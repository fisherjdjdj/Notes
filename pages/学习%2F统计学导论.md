## 统计学导论_笔记
collapsed:: true
	- 绪论
- ## 统计学导论_作业
	- ### 统计学导论_作业一
	  collapsed:: true
		- # 题目
		  collapsed:: true
		  1、统计典故/历史人物 之 我学
		  2、统计原理/科学前沿 之 我思
		  3、反直觉案例/名言思政 之 我悟
		  4、统计分析/统计应用 之 我行
			- [x] 学习[《真理与统计》]([统计与真理-C.r.劳-微信读书 (qq.com)](https://weread.qq.com/web/reader/31242403643425f376c45304565304461366e41366a32366b73444b4e445261ed4?))第1~2章
			- [x] 学习《统计学导论》第1章
			- [x] 并选择学思悟行之一收集一个案例或写一段话
			  
			  ---
		- # 作业大纲
		  * 方法：bootstrap method
		  * 数据：
		  * What is the Bootstrap?
		  * why Does it Work?
		  * Example of the Bootstrap?
		- # 素材
			- ## 1.
			- 一个从 P n P\_n 中抽取且样本量为n的样本叫做自助法样本，记作 X 1 ∗ , . . . , X n ∗ ∼ P n X\_1 ^\*, ... ,X\_n^\* \\sim P\_n 。自助法样本扮演着重要的角色。这是因为，从 P n P\_n 中抽取一个独立同分布的样本 X 1 ∗ , . . . , X n ∗ X\_1^\*, ..., X\_n^\* 等价于从原始数据 { X 1 , . . . , X n } \\{X\_1, ..., X\_n\\} 中有放回地抽取 n n 个观测值。因此，自助法往往被描述为“从数据中重抽样”。这样的描述也许有些令人疑惑，如果我们将其自助法样本 X 1 ∗ , . . . , X n ∗ X\_1^\*, ...,X\_n^\* 视作从经验分布 P n P\_n 中抽样就清晰多了。
			- ## 2.
			  自助法（bootstrap method）是一种用于估计统计量的方法，它通过从原始数据中有放回地抽取样本来构建自助样本集，然后利用自助样本集进行统计量的估计。
			  下面是一个使用Python实现自助法的示例程序：
			  
			  ```python
			  import numpy as np
			  
			  def bootstrap(data, num_iterations, statistic_func):
			    n = len(data)
			    statistics = np.zeros(num_iterations)
			    for i in range(num_iterations):
			        # 从原始数据中有放回地抽取样本，构建自助样本集
			        bootstrap_sample = np.random.choice(data, size=n, replace=True)
			        # 计算统计量
			        statistic = statistic_func(bootstrap_sample)
			        statistics[i] = statistic
			    return statistics
			  
			  # 示例统计量函数，计算样本均值
			  def mean_statistic(data):
			    return np.mean(data)
			  
			  # 示例数据
			  data = np.random.normal(loc=0, scale=1, size=1000)
			  
			  # 使用自助法估计样本均值的置信区间
			  num_iterations = 1000
			  statistics = bootstrap(data, num_iterations, mean_statistic)
			  
			  # 计算置信区间
			  confidence_interval = np.percentile(statistics, [2.5, 97.5])
			  
			  print("Bootstrap estimate of mean: {:.2f}".format(np.mean(statistics)))
			  print("95%!C(MISSING)onfidence interval: [{:.2f}, {:.2f}]".format(confidence_interval[0], confidence_interval[1]))
			  ```
			  在上面的示例程序中，首先定义了一个bootstrap函数，该函数接受三个参数：原始数据data、自助法的迭代次数num_iterations和统计量函数statistic_func。然后，在bootstrap函数中，使用np.random.choice函数从原始数据中有放回地抽取样本，构建自助样本集。接着，使用统计量函数计算自助样本集的统计量，并将统计量保存在一个数组中。最后，返回统计量数组。
			  在示例程序中，我们使用了一个示例统计量函数mean_statistic，该函数接受一个样本数据，计算样本均值。你可以根据自己的需求定义不同的统计量函数。
			  接下来，我们生成了一个随机的样本数据data，使用自助法估计样本均值的置信区间。通过调用bootstrap函数，传入样本数据、迭代次数和统计量函数，得到了一系列的样本均值估计值。然后，使用np.percentile函数计算样本均值的置信区间。
			  最后，将样本均值的估计结果和置信区间打印出来。
			  注意：自助法适用于样本数量较小的情况，对于大样本数据，可以使用其他的抽样方法，如交叉验证等。
		- # 作业：统计学小札——初探Bootstrap自助法
			- {{embed [[统计学小札——初探Bootstrap自助法]]}}
	- ### 统计学导论_作业二
	  collapsed:: true
		- # 题目：抽样调查和实验设计
			- ### 素材：
				- 抽样调查 #cards/idea
				  card-last-score:: 5
				  card-repeats:: 1
				  card-next-schedule:: 2023-09-21T00:40:57.670Z
				  card-last-interval:: 4
				  id:: 650713a9-657b-4029-a663-ceadc3ccd531
				  card-ease-factor:: 2.6
				  card-last-reviewed:: 2023-09-17T00:40:57.671Z
					- 是指从总体中选择一部分样本来代表整个总体，以便进行统计推断。抽样调查的目的是通过研究样本来了解总体的特征。
					  抽样调查可以按照不同的方法进行，如简单随机抽样、系统抽样、分层抽样等。每种方法都有其特定的优缺点和适用场景
				- 实验设计 #cards/idea
				  card-last-score:: 5
				  card-repeats:: 1
				  card-next-schedule:: 2023-09-21T00:42:51.932Z
				  card-last-interval:: 4
				  id:: 650713a9-96ab-4705-b349-8078d8aac923
				  card-ease-factor:: 2.6
				  card-last-reviewed:: 2023-09-17T00:42:51.933Z
					- 是指为了验证某个假设或研究问题而进行的有计划的实验。实验设计的目的是通过控制和观察变量之间的关系来进行因果推断。
					  在实验设计中，需要考虑实验组和对照组的设置、随机分配、样本大小、实验过程的控制等因素。
				- ![抽样调查简史_段海波.pdf](../assets/抽样调查简史_段海波_1694913944820_0.pdf)
				- 早在古巴比伦时期，在文字被发明后不久，就有关于农业普查的记载。古代的中国统计人口的数量以便确定税收和各省的军事力量。公元前，就已经有由埃及统计者编制的统计概述。在欧洲，罗马每隔一段时间就对它的人口和财产进行一次普查。这些数据被用来建立市民的政治地位和评估各州的军事力量和纳税义务。当然，当耶稣出生在伯利恒小镇的时候，犹太人也有关于人口的记载。
				- 已知的最早试图通过部分信息评价总体的人是英国人John Graunt,在他1662年出版的《观察》一书中，他描述了一种基于部分信息估算伦敦人口的方法。格朗特以伦敦城内保有较完整登记表册的教区为样本进行家庭调查。他发现平均每年每11个家庭死亡人数为3，假设这个比率对其他教区也适用，且已知每年伦敦总的死亡人数大约13000，他得出伦敦大约有48000个家庭的结论。又知平均每个家庭人数为8，他估算出伦敦人口大约为384000。尽管格朗特意识到不同时期不同地区每个家庭的平均死亡人数不相同，但他没有对这种现象作任何说明。由于他的方法缺乏恰当的科学根据，格朗特未能对他的方法的准确性加以陈述。
				- 1802年，当时法国数学家P.-S.拉普拉斯（Pierre-Simon Laplace）提出了抽样调查的概念，在一篇关于1786年巴黎人口的出生、死亡和婚姻状况的论文里，建议用某些地区的出生率来推算整个法国的人口，并对推算结果的误差问题作了研究。1802年，他在法国政府的支持下，在法国挑选了30个公社进行调查，在挑选过程中他坚持两个原则对挑选过程
				  进行控制。首先，这些公社的的选择要抵消气候差异等地区性影响，其次要求这些社区能够提供准确的信息。他对这些地区连续三年出生的人口数进行了调查，他发现平均每28.5个居民中，每年出生一个婴儿，即婴儿的出生率是3.527%，他据此推断在1812年即他的
				  《概率分析理论》一书出版时，法兰西帝国疆域以内，每年出生人数为150万，全国总人口数为4553万；他甚至还发现推算出的人口数与实际人口之间的误差为0.86%。通过使用中心极限定理，Laplace证明了他的估计服从正态分布。不幸的是，他忽视了他选用的是整群样本，而不是简单随机样本，而且公社的选取带有目的性而非随机选取。这些问题
				  使得中心极限定理的应用值得怀疑。
				- 1894年奥斯陆统计局主任凯尔在全国范围内进行的关于养老金和疾病保险计划的调查。当十年一次的普查没有也不能提供调查所要得到的详细信息时，他也就不需要执行普查收集这些信息。事实上这是一个涉及社会大众且与社会和经济密切相关的且很有优势的抽样调查。在这个调查中包含60个问题，调查者将访问120000个成年人，其中80000构成了这个调查的主体，其余40000将在工人阶级聚居的地方进行。对于这80000人的调查，受访者将成比例地分布在城镇和农村地区，这一比例根据
				  1891年进行的人口普查结果确定，于是20000个受访者分布在城镇，60000个受访者来自
				  农村地区。
				  城镇地区受访者的选取方法不同于农村地区。
				  在所有城镇中挑选出13个城镇，包括大约总共20000人的5个最大城镇。当时挪威一共有61个城镇，这就意味着有将近五分之一的城镇被选中了。较大和较小城镇的受访者数量也不是根据每个城镇的人口比例分配。由于较小城镇的总人口数多于几个较大城镇总数，于是在较小城镇选择了更多的受访者。于是，在Kristiania选中了6350人（大概占
				  其成年人口的十六分之一)，其他城镇有的选中九分之一或三分之一。Kiaer认为受访者的
				  分布方法不仅合理，而且更利于得到最终的结果。
				- 通过系统的分析，Kiaer意识到完全随机选择不是选取样本唯一且最好的方法。因此，
				  在他报告接下来的讨论中，他认为一个国家的情况可以通过选取遍布在全国的小的地理个
				  体进行研究得到。如果这些个体选取合理，它们能够代表全国。这里我们注意到对于一个
				  成功的抽样调查的两个重要条件：恰当的代表性和选择个体的合理性。
				  他用于满足代表性的方法是分层，分层的原则是根据地理条件，社会或经济状况。在
				  上面提到的两个调查中，根据地理情况进行分层，把整体分成城镇地区和农村地区，然后
				  再对它们进行细分。在城镇中，根据街道分成四种策略，他假设最少人口的街道代表了较
				  富有的层，最多人口的街道代表较贫穷的层，而中等人口的街道代表了一般经济状况的层。
				  在农村地区根据职业进行分层，确切的选择方法交给调查者决定。当然，人为选择带
				  来的偏差是不可避免的，但至少分层方法是清晰的。而且，Kiaer根据之前人口普查结果
				  在每一层进行成比例的选择。
				  全面的分层理论直到奈曼时期才得到发展，但Kiaer完全靠常识，在那时就已经应用
				  了分层方法，这种方法直到今天在选取样本方面仍然有很大的用途。
				- 英国统计学家R.A.Fisher为抽样理论也做出了重要贡献，他在罗萨姆斯特试验站工
				  id:: 65067aa5-756c-4af4-b02d-2767e3fa0019
				  作期间创建并发展了近代实验设计与方差分析理论和方法。Fisher提出随机化、重复和
				  区组这三个重要原则，这三个原则也同样对抽样理论发展提供了基础。冯士雍对这三个原
				  则有过如下描述：“随机化是获得无偏估计的基础，采用重复技术使得方差估计能够在样
				  本数据基础上得以进行，而区组的划分用于抽样即使分层，目的都是为了减少误差。”①
				  Fisher的研究主要放在在野外农田实验，这里主要的困难是：为了消除士壤肥沃性
				  上的差异，每次重复的实验要安排在与之前紧凑排列的地块上。当消除实验设计特征带来
				  的变化后，Fisher方差分析的技术使得不同方法的误差估计能够合并。为了保证误差估
				  计的有效性，他引入了随机化的原则。
				- t检验和z检验使得通过方差分析技术计算得到的误差估计用作显著性检验的基础。
				  Fisher在1935年发表的《The Design of Experiments》一文在一定程度上完成了农业
				  和生物实验的实验设计理论，并且已经令人满意的应用于经济学问题。
	- ### 统计学导论_作业三
	  collapsed:: true
		- ## 贝叶斯定理和信息论
		  collapsed:: true
			- 香农的信息论是一种数学理论，用于衡量和处理信息的传输和存储。贝叶斯定理是一种概率理论，用于更新先验概率，基于新的证据或线索来计算后验概率。
			  在贝叶斯定理中，我们开始时有一个先验概率，表示我们对某个事件或假设的初始信念。然后，当我们获得新的证据或线索时，我们使用贝叶斯定理来更新我们的信念，计算得到后验概率。
			  香农的信息论可以提供一个有趣的视角来解释贝叶斯定理的修正过程。根据香农的信息论，信息的量可以通过信息熵来度量。信息熵表示一个随机变量的不确定性或信息量。
			  当我们获得新的证据或线索时，我们可以将其视为提供了一定的信息，减少了事件的不确定性。根据香农的信息论，我们可以使用信息熵的概念来解释这个过程。
			  开始时，我们的先验概率表示了事件的初始不确定性。这可以被视为事件的初始信息熵。随着我们获得更多的证据或线索，我们的信息熵逐渐减少，因为我们的信念变得更加确定。
			  贝叶斯定理的关键是将新的证据与先验概率结合起来，以计算后验概率。这可以通过信息熵的减少来解释。新的证据提供了更多的信息，减少了事件的不确定性，从而减少了事件的信息熵。通过将先验概率与新的证据结合起来，我们可以计算出更准确的后验概率，反映了我们更新后的信念。
			  因此，可以将贝叶斯定理的不断修正过程解释为通过获得新的证据或线索，减少事件的不确定性，从而降低事件的信息熵，以更新我们的信念和概率分布。这种修正过程反映了信息论中的信息量和信息熵的概
			- 蒙提霍尔问题是一个著名的概率谜题，涉及到贝叶斯定理的应用。问题描述如下：在一个游戏中，你站在三扇门前，其中一扇门后有一辆汽车，另外两扇门后各有一只山羊。你选择一扇门，然后主持人会打开另外两扇门中的一扇，露出一只山羊。现在，你可以选择是否坚持原先的选择，或者改变选择。你应该怎么做？
			  这个问题可以通过贝叶斯定理来解释。初始时，每扇门后汽车的概率是1/3，每扇门后山羊的概率也是1/3。当主持人打开一扇门，露出山羊时，我们获得了新的证据。现在我们知道其中一扇门后肯定有山羊，这改变了我们的信息。
			- 贝叶斯定理告诉我们，要计算在新证据下每扇门后汽车的概率，我们需要将先验概率与新的证据结合起来。在这种情况下，如果你最初选择的门后有汽车（概率为1/3），主持人只会随机打开另外两扇门中的一扇，露出山羊。但如果你最初选择的是山羊（概率为2/3，因为有两扇门后是山羊），主持人只能打开另外一扇门，露出山羊。
			  因此，在这个问题中，改变选择的策略更有利于赢得汽车，因为如果你最初选择了山羊，改变选择后有2/3的概率获得汽车，而如果你最初选择了汽车，改变选择后只有1/3的概率获得汽车。这是一个经典的应用贝叶斯定理解决的问题，它展示了如何使用新的证据来更新概率分布以做出更明智的决策。
			- 从信息论的角度来分析贝叶斯定理应用于蒙提霍尔问题的过程，我们可以使用信息熵的概念。
			  在蒙提霍尔问题中，我们开始时对每扇门后的状态（汽车或山羊）存在不确定性。这可以用信息熵来表示，即初始的信息熵。
			  当我们选择一扇门后，主持人打开另外两扇门中的一扇，露出山羊。这个动作提供了新的信息，减少了事件的不确定性。我们现在知道其中一扇门后肯定有山羊。这导致了信息熵的减少。
			  根据信息论，信息熵的减少表示我们获得了更多的信息，对事件的不确定性有更多的了解。在这个问题中，我们的信息熵减少了，因为我们知道其中一扇门后是山羊。
			- 频率派和贝叶斯派在统计学中有不同的方法和观点。频率派侧重于使用频率和大样本的统计性质，而贝叶斯派强调概率是一种主观度量，可以包含先验信息。
			- 贝叶斯派认为，我们可以利用已有的先验信息来帮助我们更准确地估计参数或做出预测。这在许多实际问题中是非常有用的，因为我们经常拥有关于事件或参数的背景知识。信息论的角度可以解释为，先验信息有助于减少不确定性，从而提高我们的估计或预测的准确性。
			  在一些情况下，我们可能只有很少的数据来进行统计推断。贝叶斯方法允许我们在小样本情况下引入先验信息，从而提供更稳健的估计。这有助于避免频率派方法中可能出现的估计不稳定性问题。
			  贝叶斯方法非常灵活，可以轻松处理各种统计问题，包括参数估计、假设检验、模型选择等。通过选择不同的先验分布，我们可以适应不同的问题和数据类型，这种灵活性是贝叶斯方法的一大优势。
			  在现实世界中，我们通常会面临来自多个信息源的数据。贝叶斯方法提供了一种统一的框架，可以有效地融合不同信息源，以获得更全面的推断和决策。信息论可以用来量化这些不同信息源的贡献。
			- 因为这些特性贝叶斯方法在深度学习中有许多重要的应用，这些应用利用了概率模型和贝叶斯推断的原理来解决各种问题。在深度学习中，调整模型的超参数通常是一个挑战性问题。贝叶斯优化是一种使用贝叶斯方法来选择最佳超参数的技术。它通过构建一个高斯过程或其他概率模型来建模超参数和性能之间的关系，然后使用贝叶斯推断来选择最佳的超参数配置。深度学习模型通常用于任务如分类、回归等，但在实际应用中，对模型的不确定性估计非常重要。贝叶斯方法可以用于估计模型的预测不确定性，这对于决策制定和风险管理非常有帮助。贝叶斯方法在深度学习中提供了一种强大的工具，用于处理不确定性、超参数优化、生成模型和更多任务。它们允许研究人员更全面地理解和利用深度学习模型的性能和不确定性，从而在各种应用领域中取得更好的结果。
	- ### 统计学导论_作业四
	  id:: 6534ef0b-fc96-4d78-b235-66316f0460f9
	  collapsed:: true
		- 卷积公式：
		- 滑动平均
		- 傅里叶变换
		- ## 发展历史
		  
		  极大似然估计（Maximum Likelihood Estimation，MLE）是一种常用的参数估计方法，用于从观测数据中估计模型的参数。它的提出者是 19 世纪的英国统计学家弗朗西斯·高尔顿（Francis Galton）。
		  
		  弗朗西斯·高尔顿是一位多才多艺的科学家，他的研究领域涵盖了遗传学、气象学、地理学等多个领域。他在研究人类特征的遗传传递时，提出了极大似然估计的概念。他观察到，如果一个特征在家族中出现的概率与该特征的表现有关，那么通过比较观察数据和理论分布之间的差异，可以估计出该特征的传递概率。
		  
		  另一位对极大似然估计有重要贡献的学者是罗纳德·费雪（Ronald A. Fisher）。费雪是一位英国的统计学家和生物学家，他是现代统计学的奠基人之一。费雪在 20 世纪初提出了所谓的“最大似然法”，并将其应用于各种统计问题中。他的方法强调了使用观测数据来估计模型参数的思想，并提供了一套系统的数学框架来进行参数估计。
		  
		  弗朗西斯·高尔顿和罗纳德·费雪之间并没有直接的合作或交流，但他们的工作都为极大似然估计的发展做出了重要贡献。他们的研究成果奠定了极大似然估计在统计学中的地位，并为后来的研究者提供了重要的理论基础和方法指导。
		- ## 极大似然函数
		  
		  1. 最大化似然函数：极大似然估计的核心思想是寻找参数值，使得观测数据的似然函数取得最大值。似然函数（Likelihood function）表示给定模型下数据出现的概率。对于连续分布，似然函数通常表示为概率密度函数；对于离散分布，似然函数表示为概率质量函数。
		    
		  2. 数学公式：
			- 假设有观测数据集 X = {x1, x2, ..., xn}，这些数据是独立同分布的，即从同一个概率分布中独立采样得到。
			- 假设概率模型有一个参数 θ，我们要估计 θ 的值。
			- 似然函数 L(θ|X) 表示给定参数 θ 下观测数据 X 发生的概率。对于连续分布，似然函数通常是概率密度函数，对于离散分布，是概率质量函数。
			    
			  3. 求解方法：  
			  极大似然估计的目标是找到参数 θ 的值，使得似然函数 L(θ|X) 最大化。这可以表示为：
			  
			  $$θ_{MLE}​=argmaxL(θ∣X)$$​
			  
			  具体的计算过程将根据所选的概率模型和似然函数而异。通常，我们会取对数似然函数的最大值，因为它将乘法操作转换为加法操作，更容易处理。所以，通常采用以下形式：
			  
			  $$l(θ∣X)=logL(θ∣X)​$$
			  
			  极大似然估计的目标也变为最大化对数似然函数：
			  
			  $$θ_{MLE}​=argmaxl(θ∣X)​$$
		- ## 限制条件
		  
		  在大多数情况下，想要使用最大似然估计值的人都会被数学上的问题难住。在 20 世纪后半叶的统计学文献中，许多论文用数学上的简化方法在特定情形中巧妙地获得了最大似然估计值的良好近似值，只有将似然函数简化到某种程度，然后才能算出最大似然估计值的近似值。这一点在计算机诞生后得到了一定的解决。在大多数情况下，想要使用最大似然估计值的人都会被数学上的问题难住。在 20 世纪后半叶的统计学文献中，许多论文用数学上的简化方法在特定情形中巧妙地获得了最大似然估计值的良好近似值。在我自己的博士论文中（大约在 1966 年），我对问题的解法只能在拥有大量数据的情况下才能使用。只有根据这个假设，我才能将似然函数简化到某种程度，然后算出最大似然估计值的近似值。
		  
		  极大似然估计要求所选择的概率模型是正确的，并且观测数据服从该模型。如果模型错误，那么极大似然估计的结果可能会偏离真实值。为了解决这个问题，可以把完全未知的概率密度函数的估计转化为对参数的估计。费希尔证明，最大似然估计值总是具有一致性，而且是所有统计量中效率最高的统计量（前提是被称为“正则条件”的几个假设成立）。此外，费希尔还证明，如果最大似然估计值是有偏的，那么这个偏差可以算出来并且可以从最大似然估计值中减掉，从而得到一个一致、高效、无偏的修正统计量。这样，概率密度函数的选取就成为了关键，只有当模型正确，而且在样本区域无穷时，才能得到较准确的估计值。20 世纪 50 年代，印度的 C. R. rao 和在霍华德大学任教的戴维·布莱克威尔表示，即使费希尔的正则条件不成立，仍然可以根据最大似然估计值构造出效率最高的统计量。
		  
		  极大似然估计的计算过程可能会受到局部极值的影响，这可能导致估计结果不准确。为了解决这个问题，研究者通常采用迭代交互估计的方法来估计混合效应模型中的固定效应和随机效应（方差成分）。极大似然估计在某些情况下可能会受到异常值的影响，从而降低估计的准确性。
		  
		  值得注意的是，虽然极大似然估计有这些限制和假设条件，但在很多应用中，只要适当地选择模型和处理方法，它仍然可以提供非常准确的参数估计。
	- ### 统计学导论_作业五
	  id:: 653402f4-a11e-4726-84d7-a9cbba02fd80
	  collapsed:: true
		- # 大数据和ChatGPT
			- ## 一、什么是？——我们不妨先问问ChatGPT
				- 问：什么是大数据？#cards/question
				  logseq.order-list-type:: number
				  collapsed:: true
					- ChatGPT：大数据（Big Data）是指规模庞大、复杂多样、以及传统数据管理工具难以有效处理的数据集合。大数据通常具有以下特征：
						- **数据量大（Volume）**：大数据集合包括数十 terabytes、petabytes，甚至更多的数据。这些数据通常比传统数据库或数据仓库所能容纳的多得多。
						  logseq.order-list-type:: number
						- **数据多样性（Variety）**：大数据可以包括多种不同类型的数据，如文本、图像、音频、视频、传感器数据等。这些数据可以是结构化、半结构化或非结构化的，因此需要不同的方法来存储和分析。
						  logseq.order-list-type:: number
						- **数据速度（Velocity）**：数据可以以非常快的速度生成和传输，如社交媒体更新、传感器数据流或交易记录。因此，处理大数据需要能够实时或几乎实时地捕捉、存储和分析数据的能力。
						  logseq.order-list-type:: number
						- **数据真实性（Veracity）**：大数据可能包含不准确、不完整或不一致的数据，因此在处理大数据时需要考虑数据的质量和真实性。
						  logseq.order-list-type:: number
						- **数据价值（Value）**：大数据的价值在于能够从中提取有用的信息和洞见，帮助做出决策、进行分析或进行预测。
						  logseq.order-list-type:: number
					- 为了处理大数据，通常需要使用专门的技术和工具，包括分布式计算、大数据存储系统、数据挖掘和机器学习算法。大数据分析可以用于多个领域，包括市场营销、医疗保健、金融、科学研究、社交媒体分析、物联网等，以帮助组织更好地了解趋势、提高效率、改进决策和创新。
				- 问：什么是ChatGPT？#cards/question
				  logseq.order-list-type:: number
				  collapsed:: true
					- ChatGPT:ChatGPT是一种基于深度学习的自然语言处理模型，是GPT（Generative Pre-trained Transformer）系列的一部分，专门设计用于进行自然语言对话和生成文本。ChatGPT是经过大规模预训练的神经网络模型，具有以下主要特点：
						- **生成对话**：ChatGPT的主要用途是生成自然、流畅的文本来模拟对话，它可以用于构建虚拟助手、聊天机器人和自动问答系统。用户可以通过文字输入与ChatGPT进行交互，提出问题、寻求建议，或进行一般性的对话。
						  logseq.order-list-type:: number
						- **基于Transformer架构**：ChatGPT使用Transformer架构，这是一种深度学习模型，特别适合处理自然语言处理任务。Transformer架构的关键特点是注意力机制，它允许模型有效地捕捉文本中的语法和语义信息。
						  logseq.order-list-type:: number
						- **预训练和微调**：ChatGPT模型首先通过大规模的文本语料库进行预训练，从中学习语言知识。然后，模型可以根据具体任务进行微调，以使其适应特定应用领域或用例。
						  logseq.order-list-type:: number
						- **通用性**：ChatGPT是一个通用的自然语言处理模型，可以用于多种任务，包括回答问题、生成文本、提供建议、模拟对话等。它在多领域应用中具有广泛的潜力。
						  logseq.order-list-type:: number
						- **大规模数据集**：ChatGPT的性能取决于训练数据的质量和数量。它需要大量的文本数据来预训练，以便理解自然语言的多样性和复杂性。
						  logseq.order-list-type:: number
					- ChatGPT的一个重要特点是，它可以通过不断的微调和改进，适应不同的应用领域和用户需求。这种模型已经在自然语言处理、虚拟助手、智能客服、教育、医疗保健和其他领域中取得了广泛的应用，为用户提供更自然和智能的交互方式。
				- 问：大数据与ChatGPT有什么关系？又起了什么作用？ #cards/question
				  logseq.order-list-type:: number
				  collapsed:: true
					- ChatGPT：大数据与ChatGPT之间存在着密切的关系，并且在人工智能领域发挥着重要作用。以下是它们之间的关系以及它们在人工智能方面的应用或重要作用的简要描述：
						- 大数据与ChatGPT的关系：
						  logseq.order-list-type:: number
							- **数据训练**：ChatGPT是一种基于深度学习的自然语言处理模型，它需要大量的文本数据进行训练。大数据为这些模型提供了训练所需的庞大数据集，包括互联网上的文本、对话记录等。这些数据用于帮助ChatGPT学习语言模式、语法和语义。
							- **模型优化**：大数据还可用于优化ChatGPT的性能。通过对更多的数据进行预训练，可以改善模型的理解能力，使其更接近人类水平的自然语言处理。
						- 人工智能应用和重要作用：
						  logseq.order-list-type:: number
							- **自然语言处理**：ChatGPT在自然语言处理中发挥着关键作用。它可以用于文本生成、文本理解、情感分析、机器翻译、问答系统等任务。这些应用可以帮助企业提高客户服务、自动化文档生成等方面的效率。
							- **虚拟助手和聊天机器人**：ChatGPT可以用于构建虚拟助手和聊天机器人，用于与用户进行自然的对话。这种应用在客户支持、在线销售、健康护理等领域有广泛应用。
							- **决策支持**：ChatGPT可以分析和处理大量的文本数据，为企业和政府机构提供决策支持。它可以帮助分析社交媒体上的观点、趋势，以及从大数据中提取有用信息，帮助组织更好地了解市场、舆论和客户需求。
							- **创新研究**：ChatGPT还促进了在自然语言处理领域的创新研究。研究人员可以使用这些模型来开展各种实验和研究，推动自然语言处理的前沿。
						- 总的来说，大数据提供了训练ChatGPT所需的数据，而ChatGPT则是大数据在人工智能领域的具体应用之一，为自然语言处理和智能对话系统提供了强大的工具，促进了在多个领域的应用和创新研究。
					-
			- ## 二、不只是ChatGPT——自然语言处理模型（NLP模型）
				- NLP模型 #cards/idea
				  collapsed:: true
					- 自然语言处理（英语：Natural Language Process，简称NLP）是计算机科学、信息工程以及人工智能的子领域，专注于人机语言交互，探讨如何处理和运用自然语言。自然语言处理的研究，最早可以说开始于图灵测试，经历了以规则为基础的研究方法，流行于现在基于统计学的模型和方法，从早期的传统机器学习方法，基于高维稀疏特征的训练方式，到现在主流的深度学习方法，使用基于神经网络的低维稠密向量特征训练模型。
				- #[[Clip]] [解读NLP深度学习的各类模型 - 知乎](https://zhuanlan.zhihu.com/p/58931044)
				  最近几年，随着深度学习以及相关技术的发展，NLP领域的研究取得一个又一个突破，研究者设计各种模型和方法，来解决NLP的各类问题。下图是过去6年ACL、EMNLP、EACL和NAACL上发表深度学习长篇论文的比例逐年增加。08:52
				   ![](https://pic3.zhimg.com/v2-4dcc9fce61c7cf5171aa17724e2eb786_r.jpg)
				-
				- 除了OCR、语音识别，自然语言处理有四大类常见的任务。第一类任务：序列标注，譬如命名实体识别、语义标注、词性标注、分词等；第二类任务：分类任务，譬如文本分类、情感分析等；第三类任务：句对关系判断，譬如自然语言推理、问答QA、文本语义相似性等；第四类任务：生成式任务，譬如机器翻译、文本摘要、写诗造句、图像描述生成等。
				- 复杂的任务决定着在研究中方法使用的多元性。贝叶斯概率模型、逻辑回归、决策树、SVM、主题模型、HMM模型等，都是常见的用于NLP研究的传统机器学习算法。而深度学习是一种基于特征学习的机器学习方法，把原始数据通过简单但非线性的模块转变成更高层次、更加抽象的特征表示，通过足够多的转换组合，非常复杂的函数也能被学习。在多年的实验中，人们发现了认知的两个重要机制：抽象和迭代，从原始信号，做底层抽象，逐渐向高层抽象迭代，在迭代中抽象出更高层的模式。
				- 自然语言处理的基础研究便是人机语言交互，以机器能够理解的算法来反映人类的语言，核心是基于统计学的语言模型。语言模型（英语：Language Model，简称LM），是一串词序列的概率分布。通过语言模型，可以量化地评估一串文字存在的可能性。对于一段长度为n的文本，文本中的每个单词都有通过上文预测该单词的过程，所有单词的概率乘积便可以用来评估文本存在的可能性。在实践中，如果文本很长，$P(w_i|context(w_i))$ 的估算会很困难，因此有了简化版：N元模型。在N元模型中，通过对当前词的前N个词进行计算来估算该词的条件概率。对于N元模型。常用的有unigram、bigram和trigram，N越大，越容易出现数据稀疏问题，估算结果越不准。
				- 为了解决N元模型估算概率时的数据稀疏问题，研究者尝试用神经网络来研究语言模型。经典代表有2003年Bengio等[2]提出的NNLM，但效果并不显著。直到ChatGPT的出现，算力的发展和方案的返璞归真取得了巨大的成果，NLP模型领域迎来了巨大的革命。
			- ## 三、革命之后——传统NLP将何去何从？
				- ### 传统NLP如何被淘汰了？
				  collapsed:: true
					- 复杂推理：
					  logseq.order-list-type:: number
						- ChatGPT旨在理解上下文，并生成连贯的响应。它可以用于对话系统、聊天机器人和其他需要处理上下文的应用。
						- 传统NLP模型通常更适用于独立的文本分类或信息提取任务，对上下文的理解较弱。
					- 知识推理：
					  logseq.order-list-type:: number
						- 传统NLP模型可能包括规则、词典、统计方法等，通常不具备深度学习模型的复杂性和表达能力。
						  NLP 社区从一开始就面临着**如何有效编码知识**的挑战。人们一直在不断探究把知识保存在模型外部或者内部的方法。
						  上世纪九十年代以来，人们一直试图将语言和世界的规则记录到一个巨大的图书馆中，将知识存储在模型之外。但这是十分困难的，毕竟我们无法穷举所有规则。
						  因此，研究人员开始构建特定领域的知识库，来存储非结构化文本、半结构化（如维基百科）或完全结构化（如知识图谱）等形式的知识。通常，**结构化知识很难构建**（因为要设计知识的结构体系），**但易于推理**（因为有体系结构），**非结构化知识易于构建**（直接存起来就行），**但很难用于推理**（没有体系结构）。
						- 而ChatGPT是基于神经网络的深度学习模型，采用了变换器架构，通常使用大规模的神经网络进行训练，具有数十亿个参数。这意味着算力和存储是它学习储存知识唯一限制，而知识本身分布式储存于参数之中，同时又相互联系起来。
					- 泛化能力：
					  logseq.order-list-type:: number
						- ChatGPT在广泛的自然语言理解和生成任务上表现良好，可以处理各种语言和领域的文本。
						- 传统NLP模型通常需要针对特定任务进行定制，而且泛化性能可能较差。
					- 当然具体的模型在不同能力上也有不同的表现，单总体上简单易构带来的优势让ChatGPT拥有了超越前辈们的优秀性能。
				- ### 那么NLP死了吗？
				  collapsed:: true
					- 以ChatGPT为代表的LLM模型的成功说明了一件事：力大真的飞砖。以前某个ai方向效果不好，那第一反应当然是改进算法，改进模型，用更精妙的逻辑去处理，或者加点小trick。但是现在呢，几千亿的参数、几十个TB的数据量和可以预想到的训练所需的大到恐怖的算力可以解决一切。
					- #[[Clip]] [(12 封私信 / 77 条消息) 为什么说NLP死了？ - 知乎](https://www.zhihu.com/question/594913657)
					  前段时间，斯坦福大学大学教授Christopher D. Manning在Daedalus期刊上发表了一篇关于「人类语言理解和推理」的论文，主要梳理自然语言处理的发展历史，并分析了基础模型的未来发展前景。09:36
					- LLM模型的出现和发展是NLP领域的一个重要进展，它给NLP领域带来了新的思路和方法，为NLP的发展提供了新的动力。LLM模型的成功也证明了NLP领域的研究者们的努力取得了实质性进展，同时也说明了NLP领域仍然有很多问题需要解决。
					- 具体来说，虽然生成式AI在生成文本、自然语言生成等方面**一力降十会**，但是它也面临着很多挑战和限制。比如，生成式AI在理解文本语义、对话管理、可解释性等方面仍存在许多问题，需要进一步的研究和探索。因此，NLP领域仍然需要持续的研究和创新。
				-
	- ### 统计学导论_作业六
	  id:: 6534ef0b-e4b0-4ba4-b996-b987d2a78c43
		- 设计一个指数（国力，j力，经济，环境）能反映一个什么趋势.
		- “太空指数”反映人类对太空探索的欲望
		  考虑因素：可探测宇宙大小（望眼镜水平），发射卫星数量（各航天大国和商业公司），最大对外输出能力（看最强的火箭），探索太空人次（各航天大国和商业公司），登上热搜的太空探索事件（微博，推特），地球总体GTP（前几名之和），科幻电影数量（盗版网站 🐶）
		- ChatGPT：构建一个“太空指数”是一个非常有趣的想法，可以反映出人类对太空探索的兴趣程度。以下是关于获取相关信息以及如何量化和构成指标的建议：
			- ### 数据收集途径：
			  collapsed:: true
				- #### 1. **可探测宇宙大小（望远镜水平）**：
				  collapsed:: true
					- **信息来源：** 天文学研究机构、太空观测卫星的报告、科学期刊。
					- **数据获取途径：** 通过科学研究论文、专利文件、观测卫星的数据报告来获取相关数据。
				- #### 2. **发射卫星数量（各航天大国和商业公司）**：
				  collapsed:: true
					- **信息来源：** 国家航天局、商业航天公司的官方报告、国际航天组织。
					- **数据获取途径：** 搜集各国航天局的年度报告、商业航天公司的公开信息，以及国际航天组织的统计数据。
				- #### 3. **最大对外输出能力（最强的火箭）**：
				  collapsed:: true
					- **信息来源：** 火箭制造公司、太空科技媒体、航天机构的技术报告。
					- **数据获取途径：** 查阅各国航天机构和私人公司的技术文献，以及太空科技新闻网站的报道。
				- #### 4. **探索太空人次（各航天大国和商业公司）**：
				  collapsed:: true
					- **信息来源：** 国际航天站官方报告、各国航天局的官方网站、航天员自传、航天员培训机构。
					- **数据获取途径：** 研究官方发布的太空任务报告，航天员的自传以及相关航天机构的官方信息。
				- #### 5. **登上热搜的太空探索事件（微博，推特）**：
				  collapsed:: true
					- **信息来源：** 社交媒体平台上的热门话题、太空科技新闻网站、官方太空任务报道。
					- **数据获取途径：** 监测社交媒体平台的热门话题，关注太空科技新闻网站的报道，以及研究官方发布的太空任务信息。
				- #### 6. **地球总体GDP（前几名之和）**：
				  collapsed:: true
					- **信息来源：** 国际经济组织、世界银行、各国政府的经济统计数据。
					- **数据获取途径：** 查阅国际经济组织和世界银行的年度报告，以及各国政府的官方经济统计数据。
				- #### 7. **科幻电影数量（电影网站）**：
				  collapsed:: true
					- **信息来源：** 电影数据库网站、电影制片公司的官方数据、影视产业报告。
					- **数据获取途径：** 查询电影数据库网站的信息，获取电影制片公司的官方数据，以及研究影视产业的报告和研究。
			- ### 数据量化和指标构成：
			  collapsed:: true
				- 1. **数据量化：** 将从上述途径获得的数据转化为数字形式，比如宇宙大小可以用特定的度量单位表示，卫星数量可以用具体数字表示，太空探索事件的热搜次数可以用次数来表示，GDP可以用货币单位表示，科幻电影数量可以用具体数字表示等。
				- 2. **指标构成：** 可以为每个数据源创建一个子指标，然后将这些子指标组合成太空指数。例如：
					- **卫星指标（Satellite Index）：** 发射卫星数量 + 最大对外输出能力。
					- **探索指标（Exploration Index）：** 探索太空人次 + 登上热搜的太空探索事件次数。
					- **科技指标（Technology Index）：** 可探测宇宙大小 + 卫星指标。
					- **文化指标（Culture Index）：** 科幻电影数量 + 地球总体GDP。
					  然后，通过权重分配，将这些子指标综合起来构成太空指数。权重的分配可以基于相关性、重要性或者通过专家意见来确定。
					  请注意，为了确保指数的准确性和可靠性，建议使用最新和可靠的数据，同时也可以定期更新数据以保持指数的时效性。
				- **NASA官方网站**：[https://www.nasa.gov/](https://www.nasa.gov/)
				  可以在NASA的网站上找到有关太空探索的最新资讯和统计数据。
				  **国际航天站官方网站**：[https://www.spacestation.nasa.gov/](https://www.spacestation.nasa.gov/)
				  这是国际航天站的官方网站，提供有关太空站任务和航天员的信息。
				  **世界银行数据**：[https://data.worldbank.org/](https://data.worldbank.org/)
				  世界银行提供全球各国的GDP和其他经济数据。
				  **社交媒体平台**：例如，微博和推特。
				  您可以使用这些平台来跟踪有关太空探索的热门话题和事件。
				  **IMDb（互联网电影数据库）**：[https://www.imdb.com/](https://www.imdb.com/IMDb是一个广泛使用的电影数据库，您可以在这里找到有关科幻电影数量的信息。
		- ### 珠玉在前
			- ### **The Drake Equation**
				- $$N=R^{*} \cdot f_{p}  \cdot n_{e}  \cdot f_{l}  \cdot f_{i}  \cdot f_{c}  \cdot L$$
				  **"What do we need to know about to discover life in space?"**
				  How can we estimate the number of technological civilizations that might exist among the stars? While working as a radio astronomer at the National Radio Astronomy Observatory in Green Bank, West Virginia, Dr. Frank Drake conceived an approach to bound the terms involved in estimating the number of technological civilizations that may exist in our galaxy. The Drake Equation, as it has become known, was first presented by Drake in 1961 and identifies specific factors thought to playa role in the development of such civilizations. Although there is no unique solution to this equation, it is a generally accepted tool used by the scientific community to examine these factors
				  -- Frank Drake，1961
				- #[[Clip]] [银河系中有多少行星可能存在外星文明？——德雷克方程漫谈 - 知乎](https://zhuanlan.zhihu.com/p/24978196)
				  其中，
				  $N =银河系中可能与之进行交流的外星文明数量$
				  $R^{*} =银河系中适于智能生命发展的恒星形成的平均速率$
				  $f_{p} =拥有行星的恒星的百分比$
				  $n_{e} =每个恒星（特指可以孕育生命的恒星）系统所拥有的行星的平均数量$
				  $f_{l} =生命实际出现的合适行星的百分比$
				  $f_{i} =发展出智能生命（拥有文明）的行星的百分比$
				  $fc_{c}=开发出可将其存在的信号释放到太空技术的文明百分比$
				  $L =这些文明将可探测的信号释放到太空的时间长度$
				  08:52
			- 我们所要衡量的是历史上人类对宇宙的好奇程度
				-