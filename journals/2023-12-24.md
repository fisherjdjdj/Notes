-
- #[[Clip]] [【决策模型】马尔可夫决策过程 - 知乎](https://zhuanlan.zhihu.com/p/271221558)
  咕了2个月的时光，我终于开始更新这个专栏系列了，真的惭愧，这两天在忙着整理校长奖学金的材料，所以写完这篇文章我可能需要再咕一段时间QAQ。
  
  这次来讲一个数学模型中常见但是非常难以理解的话题，马尔可夫决策过程。可能很多同学都听过这个，那么这篇文章将带你搞懂马尔可夫性、马尔可夫链、马尔可夫过程、马尔可夫决策过程这些概念。由于本章内容有些难以理解，所以我前前后后也研究了一个多月。由于我目前的研究兴趣在于强化学习，而马尔可夫决策过程是强化学习建模的最基本模型，所以本文是根据David Silver的强化学习公开课整理的资料，马尔可夫决策过程是非常重要的模型。
  
  **1 Introduction**
  
  *   马尔可夫决策过程描述了我们想去决策事件本身的环境，即environment，我们的决策会因为环境得改变而改变，如果智能体agent选择的决策使得环境变差了，那它就会为此背锅，整个环境是持续可观察的。
  *   目前的马尔可夫决策过程，可用于相当多的行为以及模式的决策分析，也在扩展人工智能的边界，在数学建模的问题中，也可改进很多的模型。
  *   本文只讲解马尔可夫决策过程的模型分析以及概念解读与公式推导，关于模型的求解方法可利用动态规划、随机采样等，在数学建模中没必要运用深度学习法。
  
  **2 Markov Processes马尔可夫过程**
  
  **2.1 Markov Property马尔可夫性**
  
  在了解马尔可夫过程之前，我们首先得了解什么是马尔可夫性，马尔可夫性其实是一种假设，“未来的一切仅与现在有关，独立于过去的状态”。
  
  关于马尔可夫性，我们给出了如下的Definition：
  
  AstateStisMarkovifandonlyifP\\[St+1|St\]\=P\\[St+1|S1,...,St\]A\\; state\\; S\_t\\; is\\; Markov\\; if\\; and\\; only\\; if\\\\ \\mathbb\{P}\\[S\_\{t+1}\\;|\\; S\_t\]=\\mathbb\{P}\\[S\_\{t+1}\\; |\\; S\_1,...,S\_t\] \\\\A\\; state\\; S\_t\\; is\\; Markov\\; if\\; and\\; only\\; if\\\\ \\mathbb\{P}\\[S\_\{t+1}\\;|\\; S\_t\]=\\mathbb\{P}\\[S\_\{t+1}\\; |\\; S\_1,...,S\_t\] \\\\
  
  从上述的式子可以看出，t+1时刻的状态包含了1,..,t时刻状态的全部历史信息，并且当我们知道t时刻的状态后，我们只关注于环境的信息，而不用管之前所有状态的信息，这就是马尔可夫性，当论文中说某一状态或其他信息符合马尔可夫性时，我们也应当联想到这个性质。
  
  **2.2 State Transition Matrix状态传输矩阵**
  
  对于当前的马尔可夫状态s和其他的状态s'，状态传输矩阵的Definition：
  
  Pss′\=P\\[St+1\=s′|St\=s\]P\=\\[P11…P1n⋮⋮Pn1…Pnn\]\\mathcal\{P}\_\{ss'}=\\mathbb\{P}\\[S\_\{t+1}=s'\\; |\\; S\_t=s\]\\\\ \\mathcal\{P}= \\begin\{bmatrix} \\mathcal\{P}\_\{11} & \\ldots & \\mathcal\{P}\_\{1n}\\\\ \\vdots & & \\vdots \\\\ \\mathcal\{P}\_\{n1} & \\ldots & \\mathcal\{P}\_\{nn}\\\\ \\end\{bmatrix} \\\\\\mathcal\{P}\_\{ss'}=\\mathbb\{P}\\[S\_\{t+1}=s'\\; |\\; S\_t=s\]\\\\ \\mathcal\{P}= \\begin\{bmatrix} \\mathcal\{P}\_\{11} & \\ldots & \\mathcal\{P}\_\{1n}\\\\ \\vdots & & \\vdots \\\\ \\mathcal\{P}\_\{n1} & \\ldots & \\mathcal\{P}\_\{nn}\\\\ \\end\{bmatrix} \\\\
  
  以上就是状态传输矩阵的定义，大部分的模型建立都是利用矩阵的运算的，所以这部分很重要，当然ΣPss′\=1\\Sigma\\mathcal\{P\_\{ss'}}=1\\Sigma\\mathcal\{P\_\{ss'}}=1，这相信比较好理解。
  
  **2.3 Markov Chain马尔可夫链**
  
  马尔可夫链\(Markov Chain)又称马尔可夫过程\(Markov Process)，是一种无记忆的随机过程\(memoryless random process)，我们给出如下Definition，马尔可夫链是状态与转移概率的组合，AMarkovChainisatuple⟨S,P⟩A\\; Markov\\; Chain\\; is\\; a\\; tuple\\; \\langle \\mathcal\{S},\\mathcal\{P} \\rangleA\\; Markov\\; Chain\\; is\\; a\\; tuple\\; \\langle \\mathcal\{S},\\mathcal\{P} \\rangle
  
  其中，状态S\\mathcal\{S}\\mathcal\{S}是状态的集合，概率P\\mathcal\{P}\\mathcal\{P}是概率的矩阵。
  
  下面我们用David老师上课举的一个例子来进一步理解马尔可夫过程，注意这个例子贯穿了整堂课程的内容。
  
  <img src="https://pic2.zhimg.com/v2-da093b18da6f48ced0638e7e01c408a1\_b.jpg" data-caption="" data-size="normal" data-rawwidth="699" data-rawheight="606" class="origin\_image zh-lightbox-thumb" width="699" data-original="https://pic2.zhimg.com/v2-da093b18da6f48ced0638e7e01c408a1\_r.jpg"/>
  
   !\[]\(https://pic2.zhimg.com/80/v2-da093b18da6f48ced0638e7e01c408a1_720w.webp) 
  
  这是一个学生上课情况的马尔可夫链结构，我们将Class 1作为学生的起始状态，把Sleep作为学生的终止状态。那我们可以想到一个学生从开始到结束可能会经历各种不同可能的状态。可能刷Facebook上瘾了出不来了，可能去Pub蹦迪了又返回复习了第一节课，等等。
  
  那我们接下来首先写出这条马尔可夫链的状态传输矩阵：
  
  P\=\\[0.50.50.80.20.60.41.00.20.40.40.10.91\]\\mathcal\{P}= \\begin\{bmatrix} & 0.5 & & & & 0.5 & \\\\ & & 0.8 & & & & 0.2\\\\ & & & 0.6 & 0.4 & & \\\\ & & & & & & 1.0\\\\ 0.2 & 0.4 & 0.4 & & & & \\\\ 0.1 & & & & & 0.9 & \\\\ & & & & & & 1\\\\ \\end\{bmatrix} \\\\\\mathcal\{P}= \\begin\{bmatrix} & 0.5 & & & & 0.5 & \\\\ & & 0.8 & & & & 0.2\\\\ & & & 0.6 & 0.4 & & \\\\ & & & & & & 1.0\\\\ 0.2 & 0.4 & 0.4 & & & & \\\\ 0.1 & & & & & 0.9 & \\\\ & & & & & & 1\\\\ \\end\{bmatrix} \\\\
  
  其中行列分别代表状态C1,C2,C3,Pass,Pub,Facebook,Sleep。
  
  **3 Markov Reward Process马尔可夫奖励过程**
  
  **3.1 MRP**
  
  简单来说，马尔可夫奖励过程就是含有奖励的马尔可夫链，要想理解MRP方程的含义，我们就得弄清楚奖励函数的由来，我们可以把奖励表述为进入某一状态后收获的奖励。奖励函数如下所示：
  
  Rs\=E\\[Rt+1|St\=s\]\\mathcal\{R}\_s=\\mathbb\{E}\\[R\_\{t+1}\\; | \\; S\_t=s\] \\\\\\mathcal\{R}\_s=\\mathbb\{E}\\[R\_\{t+1}\\; | \\; S\_t=s\] \\\\
  
  其实看到这个公式，我相信很多读者应该和我一样纠结，为什么奖励是t+1下一时刻的呢？然而这只是一个约定而已，实际上你把Rt+1R\_\{t+1}R\_\{t+1}改成RtR\_tR\_t也没什么关系。本质上还是我们上面说的“把奖励表述为进入某一状态后收获的奖励”，这样理解即可。
  
  下面就能给出MRP的Definition了，AMarkovrewardprocessisatuple⟨S,P,R,γ⟩A\\; Markov\\; reward\\; process\\;is\\; a\\; tuple\\; \\langle \\mathcal\{S},\\mathcal\{P},\\mathcal\{R},\\mathcal\{\\gamma} \\rangleA\\; Markov\\; reward\\; process\\;is\\; a\\; tuple\\; \\langle \\mathcal\{S},\\mathcal\{P},\\mathcal\{R},\\mathcal\{\\gamma} \\rangle
  
  其中，S还是状态合集，P是概率传输矩阵，R是奖励函数如上所示，γ\\gamma\\gamma是衰减因子。
  
  衰减因子是金融学上的概念，代表了对于远期利益的不确定性，其中γ∈\\[0,1\]\\gamma \\in\\[0,1\]\\gamma \\in\\[0,1\]，下面讲到回报时还会提到。
  
  **3.2 Return回报**
  
  回报的定义是从当前时刻开始的回报与衰减因子的乘积之和，公式如下：
  
  Gt\=Rt+1+γRt+2+...\=∑k\=0∞γkRt+k+1G\_t = R\_\{t+1}+\\gamma R\_\{t+2}+...=\\sum\_\{k=0}^\\infty \\gamma^k R\_\{t+k+1} \\\\G\_t = R\_\{t+1}+\\gamma R\_\{t+2}+...=\\sum\_\{k=0}^\\infty \\gamma^k R\_\{t+k+1} \\\\
  
  注意回报的定义并不是当前状态之前的奖励总和，我们一定要理解马尔科夫模型构建的意义，是为了探寻未来的最优策略，以及马尔可夫性与历史总是不相关的，仅与当前状态有关。所以一切模型构建均是围绕未来进行展开的，包括这里的回报。衰减因子代表人们对于未来奖励的期望，如果γ\\gamma\\gamma趋近0，则更重视眼前的利益，如果γ\\gamma\\gamma趋近1，则更重视未来的利益。
  
  **3.3 Value Function价值函数**
  
  价值函数的定义是当处于现在状态s时，MRP未来回报的期望值，价值函数给出了当前状态的长期价值。
  
  v\(s)\=E\\[Gt|St\=s\]v\(s)=\\mathbb\{E}\\[G\_t\\; |\\; S\_t=s\] \\\\v\(s)=\\mathbb\{E}\\[G\_t\\; |\\; S\_t=s\] \\\\
  
  下面说了这么多概念让我们回到刚刚学生上课的例子。
  
  <img src="https://pic3.zhimg.com/v2-612635d075f1574523a2671ce2d63052\_b.jpg" data-caption="" data-size="normal" data-rawwidth="732" data-rawheight="634" class="origin\_image zh-lightbox-thumb" width="732" data-original="https://pic3.zhimg.com/v2-612635d075f1574523a2671ce2d63052\_r.jpg"/>
  
   !\[]\(https://pic3.zhimg.com/80/v2-612635d075f1574523a2671ce2d63052_720w.webp) 
  
  在上图中，圆圈代表状态，R代表进入圆圈状态的即时奖励，而圆圈内的红色数字代表的就是价值函数，如何求出价值函数也就是当前马尔可夫最重要也是最需要讨论的问题。
  
  **3.4 Bellman Equation贝尔曼方程**
  
  要想求解马尔可夫奖励过程的价值函数，我们在这里引入了贝尔曼方程，首先让我们看看贝尔曼方程：
  
  v\(s)\=E\\[Gt|St\=s\]\=E\\[Rt+1+γRt+2+γ2Rt+3+⋯|St\=s\]\=E\\[Rt+1+γ\(Rt+2+γRt+3+⋯)|St\=s\]\=E\\[Rt+1+γGt+1|St\=s\]\=E\\[Rt+1+γv\(St+1)|St\=s\]\\begin\{align} v\(s) &= \\mathbb\{E}\\[G\_t\\; |\\;S\_t=s\]\\\\ & = \\mathbb\{E}\\[R\_\{t+1} + \\gamma R\_\{t+2} +\\gamma^2 R\_\{t+3}+\\cdots \\; | \\; S\_t=s\]\\\\ & = \\mathbb\{E}\\[R\_\{t+1} + \\gamma \(R\_\{t+2} +\\gamma R\_\{t+3}+\\cdots \\;) | \\; S\_t=s\]\\\\ & = \\mathbb\{E}\\[R\_\{t+1} + \\gamma G\_\{t+1}\\; |\\; S\_t=s\]\\\\ & = \\mathbb\{E}\\[R\_\{t+1} + \\gamma v\(S\_\{t+1})\\; |\\; S\_t=s\] \\end\{align} \\\\\\begin\{align} v\(s) &= \\mathbb\{E}\\[G\_t\\; |\\;S\_t=s\]\\\\ & = \\mathbb\{E}\\[R\_\{t+1} + \\gamma R\_\{t+2} +\\gamma^2 R\_\{t+3}+\\cdots \\; | \\; S\_t=s\]\\\\ & = \\mathbb\{E}\\[R\_\{t+1} + \\gamma \(R\_\{t+2} +\\gamma R\_\{t+3}+\\cdots \\;) | \\; S\_t=s\]\\\\ & = \\mathbb\{E}\\[R\_\{t+1} + \\gamma G\_\{t+1}\\; |\\; S\_t=s\]\\\\ & = \\mathbb\{E}\\[R\_\{t+1} + \\gamma v\(S\_\{t+1})\\; |\\; S\_t=s\] \\end\{align} \\\\
  
  这就是贝尔曼方程简单的推导过程，这样我们就可以把价值函数分为两部分，一部分是即时奖励R，一部分是计算损失的下一状态的价值函数。
  
  下面，我们抛开时间的关系，仅由状态来列写上述方程：
  
  v\(s)\=E\\[Rt+1+γv\(St+1)|St\=s\]v\(s)\=Rs+γ∑s′∈SPss′v\(s′)v\(s) = \\mathbb\{E}\\[R\_\{t+1}+\\gamma v\(S\_\{t+1})\\; |\\; S\_t=s\]\\\\ v\(s) = R\_s + \\gamma \\sum\_\{s'\\in \\mathcal\{S}}\\mathcal\{P}\_\{ss'}v\(s') \\\\v\(s) = \\mathbb\{E}\\[R\_\{t+1}+\\gamma v\(S\_\{t+1})\\; |\\; S\_t=s\]\\\\ v\(s) = R\_s + \\gamma \\sum\_\{s'\\in \\mathcal\{S}}\\mathcal\{P}\_\{ss'}v\(s') \\\\
  
  以上两式就是贝尔曼方程的两种形式了，下面我们来进行求解，将贝尔曼方程简化为矩阵形式：
  
  v\=R+γPv\\[v\(1)⋮v\(n)\]\=\\[R1⋮Rn\]+γ\\[P11…P1n⋮Rn…Pnn\]\\[v\(1)⋮v\(n)\]v = \\mathcal\{R}+\\gamma \\mathcal\{P}v\\\\ \\begin\{bmatrix} v\(1)\\\\ \\vdots\\\\ v\(n) \\end\{bmatrix} = \\begin\{bmatrix} \\mathcal\{R}\_1\\\\ \\vdots\\\\ \\mathcal\{R}\_n \\end\{bmatrix} + \\gamma \\begin\{bmatrix} \\mathcal\{P}\_\{11}&\\ldots&\\mathcal\{P}\_\{1n}\\\\ \\vdots\\\\ \\mathcal\{R}\_n&\\ldots&\\mathcal\{P}\_\{nn} \\end\{bmatrix} \\begin\{bmatrix} v\(1)\\\\ \\vdots\\\\ v\(n) \\end\{bmatrix} \\\\v = \\mathcal\{R}+\\gamma \\mathcal\{P}v\\\\ \\begin\{bmatrix} v\(1)\\\\ \\vdots\\\\ v\(n) \\end\{bmatrix} = \\begin\{bmatrix} \\mathcal\{R}\_1\\\\ \\vdots\\\\ \\mathcal\{R}\_n \\end\{bmatrix} + \\gamma \\begin\{bmatrix} \\mathcal\{P}\_\{11}&\\ldots&\\mathcal\{P}\_\{1n}\\\\ \\vdots\\\\ \\mathcal\{R}\_n&\\ldots&\\mathcal\{P}\_\{nn} \\end\{bmatrix} \\begin\{bmatrix} v\(1)\\\\ \\vdots\\\\ v\(n) \\end\{bmatrix} \\\\
  
  下面进行线性方程的矩阵直接求解：
  
  v\=R+γPv\(1−γP)\=Rv\=\(1−γP)R\\begin\{align} v &= \\mathcal\{R}+\\gamma \\mathcal\{P}v\\\\ \(1-\\gamma \\mathcal\{P})&= \\mathcal\{R}\\\\ v &= \(1-\\gamma \\mathcal\{P})\\mathcal\{R} \\end\{align} \\\\\\begin\{align} v &= \\mathcal\{R}+\\gamma \\mathcal\{P}v\\\\ \(1-\\gamma \\mathcal\{P})&= \\mathcal\{R}\\\\ v &= \(1-\\gamma \\mathcal\{P})\\mathcal\{R} \\end\{align} \\\\
  
  当然这种直接解法只能适用于小型的MRP模型，大型的MRP模型通常采用迭代的方法，比如动态规划，蒙特卡洛评估，时序差分学习等等。
  
  **4 Markov Decision Process马尔可夫决策过程**
  
  **4.1 MDP**
  
  下面终于讲到了今天的重头戏，MDP模型，如模型标题的意思所言，MDP就是具有决策状态的马尔可夫奖励过程。这里我们直接给出了马尔可夫决策过程的定义：AMarkovDecisionProcessisatuple⟨S,A,P,R,γ⟩A\\; Markov\\; Decision\\; Process\\; is \\; a\\; tuple\\; \\langle \\mathcal\{S},\\mathcal\{A},\\mathcal\{P},\\mathcal\{R},\\mathcal\{\\gamma} \\rangleA\\; Markov\\; Decision\\; Process\\; is \\; a\\; tuple\\; \\langle \\mathcal\{S},\\mathcal\{A},\\mathcal\{P},\\mathcal\{R},\\mathcal\{\\gamma} \\rangle
  
  显然比起马尔可夫奖励过程，我们多了一个A\\mathcal\{A}\\mathcal\{A}集合代表决策过程中所有action的集合。
  
  而相应的，我们也需要改动传输概率矩阵和奖励函数的定义式了，因为他们都需要与action有关了。
  
  Pss′a\=P\\[St+1\=s′|St\=s,At\=a\]Rsa\=E\\[Rt+1|St\=s,At\=a\]\\mathcal\{P}\_\{ss'}^a = \\mathbb\{P}\\[S\_\{t+1}=s'\\; |\\; S\_t=s,A\_t=a\]\\\\ \\mathcal\{R}\_\{s}^a = \\mathbb\{E}\\[R\_\{t+1}\\; |\\; S\_t=s,A\_t=a\] \\\\\\mathcal\{P}\_\{ss'}^a = \\mathbb\{P}\\[S\_\{t+1}=s'\\; |\\; S\_t=s,A\_t=a\]\\\\ \\mathcal\{R}\_\{s}^a = \\mathbb\{E}\\[R\_\{t+1}\\; |\\; S\_t=s,A\_t=a\] \\\\
  
  下面让我们回到那个学生上课的例子：
  
  <img src="https://pic3.zhimg.com/v2-2beeba4c0e46d77f63070918f7cec3c6\_b.jpg" data-caption="" data-size="normal" data-rawwidth="739" data-rawheight="594" class="origin\_image zh-lightbox-thumb" width="739" data-original="https://pic3.zhimg.com/v2-2beeba4c0e46d77f63070918f7cec3c6\_r.jpg"/>
  
   !\[]\(https://pic3.zhimg.com/80/v2-2beeba4c0e46d77f63070918f7cec3c6_720w.webp) 
  
  在MDP模型中，我们的智能体agent是能选择自己行动的action的，如果环境因为他的action变差了，那它就会因此背锅，最终的选择一定是让环境越来越好。
  
  **4.2 Policies策略**
  
  策略是agent对于环境所表达的行为，这里我们给出它的定义和解释：
  
  π\(a|s)\=P\\[At\=a|St\=s\]\\pi\(a|s)=\\mathbb\{P}\\[A\_t=a\\;|\\; S\_t=s\] \\\\\\pi\(a|s)=\\mathbb\{P}\\[A\_t=a\\;|\\; S\_t=s\] \\\\
  
  在上述定义中，MDP模型依旧是所有状态保持马尔可夫性，则我们可以得出MDP的策略也只与当前状态有关，与历史无关。另外策略也是与时间无关的，仅与当前状态有关，即At∼π\(⋅|St),∀t\>0A\_t\\sim \\pi\(\\cdot|S\_t),\\forall t>0A\_t\\sim \\pi\(\\cdot|S\_t),\\forall t>0
  
  下面我们来理解策略在MDP的作用，以及MDP模型的分解：
  
  *   给出一个MDP M\=⟨S,A,P,R,γ⟩\\mathcal\{M}=\\langle \\mathcal\{S},\\mathcal\{A},\\mathcal\{P},\\mathcal\{R},\\mathcal\{\\gamma} \\rangle\\mathcal\{M}=\\langle \\mathcal\{S},\\mathcal\{A},\\mathcal\{P},\\mathcal\{R},\\mathcal\{\\gamma} \\rangle 和一个策略 π\\pi\\pi
  *   此时状态序列就构成了马尔可夫链 ⟨S,Pπ⟩\\langle \\mathcal\{S},\\mathcal\{P}^\\pi \\rangle\\langle \\mathcal\{S},\\mathcal\{P}^\\pi \\rangle
  *   此时状态与奖励构成了MRP ⟨S,Pπ,Rπ,γ⟩\\langle \\mathcal\{S},\\mathcal\{P}^\\pi,\\mathcal\{R}^\\pi,\\mathcal\{\\gamma} \\rangle\\langle \\mathcal\{S},\\mathcal\{P}^\\pi,\\mathcal\{R}^\\pi,\\mathcal\{\\gamma} \\rangle
  
  而这里我们给出基于策略 π\\pi\\pi 的传输概率矩阵与奖励，即新的定义：
  
  Pss′π\=∑a∈Aπ\(a|s)Pss′aRsπ\=∑a∈Aπ\(a|s)Rsa\\mathcal\{P}\_\{ss'}^\{\\pi}=\\sum\_\{a\\in \\mathcal\{A}}\\pi\(a|s)\\mathcal\{P}\_\{ss'}^a\\\\ \\mathcal\{R}\_s^\{\\pi} = \\sum\_\{a\\in \\mathcal\{A} }\\pi\(a|s)R\_s^a \\\\\\mathcal\{P}\_\{ss'}^\{\\pi}=\\sum\_\{a\\in \\mathcal\{A}}\\pi\(a|s)\\mathcal\{P}\_\{ss'}^a\\\\ \\mathcal\{R}\_s^\{\\pi} = \\sum\_\{a\\in \\mathcal\{A} }\\pi\(a|s)R\_s^a \\\\
  
  **4.3 Policy based Value Function基于策略的价值函数**
  
  MDP模型中有两种基于策略的价值函数：\(1) 在状态s时收益的期望，代表的是状态带来的价值 \(2) 在状态s时，采取动作a后收益的期望，代表的是动作带来的价值。两者共同构成了MDP的价值函数。
  
  下面我们分别给出定义式：
  
  *   state-value function
  
  vπ\(s)\=Eπ\\[Gt|St\=s\]v\_\{\\pi}\(s)=\\mathbb\{E}\_\\pi\\[G\_t\\; |\\; S\_t=s\] \\\\v\_\{\\pi}\(s)=\\mathbb\{E}\_\\pi\\[G\_t\\; |\\; S\_t=s\] \\\\
  
  *   action-value function
  
  qπ\(s,a)\=Eπ\\[Gt|St\=s,At\=a\]q\_\\pi\(s,a)=\\mathbb\{E}\_\\pi\\[G\_t\\; |\\; S\_t=s,A\_t=a\] \\\\q\_\\pi\(s,a)=\\mathbb\{E}\_\\pi\\[G\_t\\; |\\; S\_t=s,A\_t=a\] \\\\
  
  **4.4 Bellman Expectation Equation贝尔曼期望方程**
  
  同样，我们依旧可以利用贝尔曼方程来转换两个基于策略的价值函数：
  
  vπ\(s)\=Eπ\\[Rt+1+γvπ\(St+1)|St\=s\]qπ\(s,a)\=Eπ\\[Rt+1+γqπ\(St+1,At+1)|St\=s,At\=a\]v\_\{\\pi}\(s)=\\mathbb\{E}\_\\pi\\[R\_\{t+1} + \\gamma v\_\\pi \(S\_\{t+1})\\; |\\; S\_t=s\]\\\\ q\_\\pi\(s,a)=\\mathbb\{E}\_\\pi\\[R\_\{t+1} + \\gamma q\_\\pi\(S\_\{t+1},A\_\{t+1})\\; |\\; S\_t=s,A\_t=a\] \\\\v\_\{\\pi}\(s)=\\mathbb\{E}\_\\pi\\[R\_\{t+1} + \\gamma v\_\\pi \(S\_\{t+1})\\; |\\; S\_t=s\]\\\\ q\_\\pi\(s,a)=\\mathbb\{E}\_\\pi\\[R\_\{t+1} + \\gamma q\_\\pi\(S\_\{t+1},A\_\{t+1})\\; |\\; S\_t=s,A\_t=a\] \\\\
  
  下面我们需要将我们的MDP模型进行解释，在马尔可夫链和MRP模型中，我们仅有状态的价值函数，那很显然在MDP模型中这是不够的，因为我们引入了Action，如果环境因为我们agent的Action而变差了，那Action则一定要背锅，于是我们也要考虑Action的价值函数，下面我们根据各种情况分别阐述如何求出价值函数。
  
  *   从状态到动作的价值函数
  
  <img src="https://pic2.zhimg.com/v2-207e52e4eb4782453f3fbb73250d3ec9\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1105" data-rawheight="209" class="origin\_image zh-lightbox-thumb" width="1105" data-original="https://pic2.zhimg.com/v2-207e52e4eb4782453f3fbb73250d3ec9\_r.jpg"/>
  
   !\[]\(https://pic2.zhimg.com/80/v2-207e52e4eb4782453f3fbb73250d3ec9_720w.webp) 
  
  当我们处于状态s时，agent有两个状态可以去执行，那我们此处的价值函数可以定义为：
  
  vπ\(s)\=∑a∈Aπ\(a|s)qπ\(s,a)v\_\\pi\(s)=\\sum\_\{a\\in \\mathcal\{A}}\\pi\(a|s)q\_\\pi\(s,a) \\\\v\_\\pi\(s)=\\sum\_\{a\\in \\mathcal\{A}}\\pi\(a|s)q\_\\pi\(s,a) \\\\
  
  这其实也很好理解，状态的价值函数就是所有下一步执行动作的价值函数的数学期望。
  
  *   从动作到状态的价值函数
  
  <img src="https://pic3.zhimg.com/v2-f69d945249024c55a0ed4941a216ea3e\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1109" data-rawheight="252" class="origin\_image zh-lightbox-thumb" width="1109" data-original="https://pic3.zhimg.com/v2-f69d945249024c55a0ed4941a216ea3e\_r.jpg"/>
  
   !\[]\(https://pic3.zhimg.com/80/v2-f69d945249024c55a0ed4941a216ea3e_720w.webp) 
  
  当从状态s执行动作action后我们可以进入下一个状态s‘，那我们此处的价值函数可以定义为：
  
  qπ\(s,a)\=Rsa+γ∑s′∈SPss′avπ\(s′)q\_\\pi\(s,a)=\\mathcal\{R}\_s^a+\\gamma \\sum\_\{s'\\in \\mathcal\{S}}\\mathcal\{P}\_\{ss'}^av\_\\pi\(s') \\\\q\_\\pi\(s,a)=\\mathcal\{R}\_s^a+\\gamma \\sum\_\{s'\\in \\mathcal\{S}}\\mathcal\{P}\_\{ss'}^av\_\\pi\(s') \\\\
  
  动作的价值函数就是离开状态s的即时奖励，加上所有可以进入下一个状态概率与价值的和。
  
  *   从状态到状态的价值函数
  
  <img src="https://pic4.zhimg.com/v2-93638f9cf3cf0454523f3b855656db73\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1103" data-rawheight="284" class="origin\_image zh-lightbox-thumb" width="1103" data-original="https://pic4.zhimg.com/v2-93638f9cf3cf0454523f3b855656db73\_r.jpg"/>
  
   !\[]\(https://pic4.zhimg.com/80/v2-93638f9cf3cf0454523f3b855656db73_720w.webp) 
  
  状态到状态的价值函数只需要把上面两个价值函数合在一块即可：
  
  vπ\(s)\=∑a∈Aπ\(a|s)\(Rsa+γ∑s′∈SPss′avπ\(s′))v\_\\pi\(s)=\\sum\_\{a\\in \\mathcal\{A}}\\pi\(a|s)\\left\(\\mathcal\{R}\_s^a+\\gamma \\sum\_\{s'\\in \\mathcal\{S}}\\mathcal\{P}\_\{ss'}^av\_\\pi\(s')\\right) \\\\v\_\\pi\(s)=\\sum\_\{a\\in \\mathcal\{A}}\\pi\(a|s)\\left\(\\mathcal\{R}\_s^a+\\gamma \\sum\_\{s'\\in \\mathcal\{S}}\\mathcal\{P}\_\{ss'}^av\_\\pi\(s')\\right) \\\\
  
  *   从动作到动作的价值函数
  
  <img src="https://pic2.zhimg.com/v2-37f4456fa1002ff1e2f98a7fa4b8dced\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1113" data-rawheight="265" class="origin\_image zh-lightbox-thumb" width="1113" data-original="https://pic2.zhimg.com/v2-37f4456fa1002ff1e2f98a7fa4b8dced\_r.jpg"/>
  
   !\[]\(https://pic2.zhimg.com/80/v2-37f4456fa1002ff1e2f98a7fa4b8dced_720w.webp) 
  
  qπ\(s,a)\=Rsa+γ∑s′∈SPss′a∑a′∈Aπ\(a′|s′)qπ\(s′,a′)q\_\\pi\(s,a)=\\mathcal\{R}\_s^a+\\gamma \\sum\_\{s'\\in \\mathcal\{S}}\\mathcal\{P}\_\{ss'}^a\\sum\_\{a'\\in \\mathcal\{A}}\\pi\(a'|s')q\_\\pi\(s',a') \\\\q\_\\pi\(s,a)=\\mathcal\{R}\_s^a+\\gamma \\sum\_\{s'\\in \\mathcal\{S}}\\mathcal\{P}\_\{ss'}^a\\sum\_\{a'\\in \\mathcal\{A}}\\pi\(a'|s')q\_\\pi\(s',a') \\\\
  
  下面举一个简单的计算例子，还是使用学生上课的MDP模型。
  
  <img src="https://pic4.zhimg.com/v2-e8640a6af549f60f315fb3139938e87b\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1099" data-rawheight="658" class="origin\_image zh-lightbox-thumb" width="1099" data-original="https://pic4.zhimg.com/v2-e8640a6af549f60f315fb3139938e87b\_r.jpg"/>
  
   !\[]\(https://pic4.zhimg.com/80/v2-e8640a6af549f60f315fb3139938e87b_720w.webp) 
  
  假设我们通过某些算法求得红色的状态的价值为7.4，那我们该如何进行验证？这里可能会有读者很奇怪，这个价值函数不是求出来的嘛，为啥是验证。这些价值函数都只能得到一个近似解，那么所有近似解都涉及到了损失值的问题，那我们就需要去拟合最小的Loss，当然现在流行深度强化学习方法。
  
  好，话说回来，根据我们上面状态到状态的价值函数，就可以很明确的得出这个值了，计算公式如图红色部分所示。下面我们列出MDP模型的贝尔曼方程矩阵形式，并给出直接解。
  
  vπ\=Rπ+γPπvπvπ\=\(1−γPπ)−1Rπv\_\\pi=\\mathcal\{R}^\\pi+\\gamma \\mathcal\{P}^\\pi v\_\\pi \\\\ v\_\\pi =\(1-\\gamma \\mathcal\{P}^\\pi)^\{-1}\\mathcal\{R}^\\pi \\\\v\_\\pi=\\mathcal\{R}^\\pi+\\gamma \\mathcal\{P}^\\pi v\_\\pi \\\\ v\_\\pi =\(1-\\gamma \\mathcal\{P}^\\pi)^\{-1}\\mathcal\{R}^\\pi \\\\
  
  **4.5 Optimal Value Function最优价值函数**
  
  下面我们分别给出状态和动作最优价值函数的定义，其实就是在策略 π\\pi\\pi 下，可以取得最大的价值函数，因为由于agent选择的不同策略，所有状态的价值函数都会相应的改变。
  
  v∗\(s)\=maxπvπ\(s)q∗\(s,a)\=maxπqπ\(s,a)v\_\*\(s)=\\max\_\\pi v\_\\pi\(s)\\\\ q\_\*\(s,a)=\\max\_\\pi q\_\\pi\(s,a) \\\\v\_\*\(s)=\\max\_\\pi v\_\\pi\(s)\\\\ q\_\*\(s,a)=\\max\_\\pi q\_\\pi\(s,a) \\\\
  
  所有的MDP模型的最终任务就是为了确定最优价值函数相应的策略。
  
  **4.6 Theorem of MDP定理**
  
  以上给出的绝大部分是定义，下面我们给出几条MDP模型的定理，注意这是定理。
  
  *   对于MDP模型，我们承认存在一个最优策略π∗\\pi\_\*\\pi\_\*，比其他任何策略都好，即π∗≥π,∀π\\pi\_\* \\geq \\pi, \\forall \\pi\\pi\_\* \\geq \\pi, \\forall \\pi，这里我们还需要再定义策略比较的规则，即π≥π′ifvπ\(s)≥vπ′\(s),∀s\\pi \\geq \\pi' \\; if\\; v\_\\pi\(s)\\geq v\_\{\\pi'}\(s),\\forall s\\pi \\geq \\pi' \\; if\\; v\_\\pi\(s)\\geq v\_\{\\pi'}\(s),\\forall s
  *   所有的最优策略都有相同的最优价值函数，即vπ∗\(s)\=v∗\(s)v\_\{\\pi\_\*}\(s)=v\_\*\(s)v\_\{\\pi\_\*}\(s)=v\_\*\(s)
  *   所有的最优策略都有相同的最优动作价值函数，即qπ∗\(s,a)\=q∗\(s,a)q\_\{\\pi\_\*}\(s,a)=q\_\{\*}\(s,a)q\_\{\\pi\_\*}\(s,a)=q\_\{\*}\(s,a)
  
  **4.7 Finding an Optimal Policy寻找最优策略**
  
  我们可以通过最大化动作价值函数q∗\(s,a)q\_\{\*}\(s,a)q\_\{\*}\(s,a)的方法来寻找最优策略：
  
  π∗\(a|s)\=\{1if a\=arg⁡maxa∈Aq∗\(s,a)0otherwise \\pi\_\*\(a|s)=\\left\\\{ \\begin\{array}\{rl} 1 & \\text\{if } \\displaystyle a=\\arg\\max\_\{a\\in \\mathcal\{A}} q\_\*\(s,a)\\\\ 0 & \\text\{otherwise } \\end\{array} \\right. \\\\\\pi\_\*\(a|s)=\\left\\\{ \\begin\{array}\{rl} 1 & \\text\{if } \\displaystyle a=\\arg\\max\_\{a\\in \\mathcal\{A}} q\_\*\(s,a)\\\\ 0 & \\text\{otherwise } \\end\{array} \\right. \\\\
  
  **4.8 Bellman Optimality Equation贝尔曼最优方程**
  
  <img src="https://pic2.zhimg.com/v2-d3d047849cc489bd867ae5db8be4f905\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1113" data-rawheight="228" class="origin\_image zh-lightbox-thumb" width="1113" data-original="https://pic2.zhimg.com/v2-d3d047849cc489bd867ae5db8be4f905\_r.jpg"/>
  
   !\[]\(https://pic2.zhimg.com/80/v2-d3d047849cc489bd867ae5db8be4f905_720w.webp) 
  
  当我们的agent在状态s时，对于该状态的最优价值函数一定是选择价值最优的动作函数，即：
  
  v∗\(s)\=maxaq∗\(s,a)v\_\*\(s)=\\max\_a q\_\*\(s,a) \\\\v\_\*\(s)=\\max\_a q\_\*\(s,a) \\\\
  
  而对于action的最优动作价值而言，相当于离开状态s的即时奖励，加上可以转移的下一个状态的最优价值与传输概率的成绩之和，即：
  
  <img src="https://pic3.zhimg.com/v2-f69d945249024c55a0ed4941a216ea3e\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1109" data-rawheight="252" class="origin\_image zh-lightbox-thumb" width="1109" data-original="https://pic3.zhimg.com/v2-f69d945249024c55a0ed4941a216ea3e\_r.jpg"/>
  
   !\[]\(https://pic3.zhimg.com/80/v2-f69d945249024c55a0ed4941a216ea3e_720w.webp) 
  
  q∗\(s,a)\=Rsa+γ∑s′∈SPss′av∗\(s′)q\_\*\(s,a)=\\mathcal\{R}\_s^a+\\gamma\\sum\_\{s'\\in \\mathcal\{S}}\\mathcal\{P}\_\{ss'}^av\_\*\(s') \\\\q\_\*\(s,a)=\\mathcal\{R}\_s^a+\\gamma\\sum\_\{s'\\in \\mathcal\{S}}\\mathcal\{P}\_\{ss'}^av\_\*\(s') \\\\
  
  那么状态到状态的价值函数与动作到动作的价值函数与上面同理，只需要把两个方程式叠加即可。
  
  <img src="https://pic3.zhimg.com/v2-9d8d1c8116bb6c693e4fe5e6bd56328a\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1018" data-rawheight="453" class="origin\_image zh-lightbox-thumb" width="1018" data-original="https://pic3.zhimg.com/v2-9d8d1c8116bb6c693e4fe5e6bd56328a\_r.jpg"/>
  
   !\[]\(https://pic3.zhimg.com/80/v2-9d8d1c8116bb6c693e4fe5e6bd56328a_720w.webp) 
  
  <img src="https://pic2.zhimg.com/v2-d334368910870cd80a3a5ddfb3cd2f51\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1016" data-rawheight="408" class="origin\_image zh-lightbox-thumb" width="1016" data-original="https://pic2.zhimg.com/v2-d334368910870cd80a3a5ddfb3cd2f51\_r.jpg"/>
  
   !\[]\(https://pic2.zhimg.com/80/v2-d334368910870cd80a3a5ddfb3cd2f51_720w.webp) 
  
  最后再举一个计算最优方程的例子，让我们再回到学生上课的MDP模型中。
  
  <img src="https://pic3.zhimg.com/v2-cc8a149fd049cfe864b094cdfb034992\_b.jpg" data-caption="" data-size="normal" data-rawwidth="1084" data-rawheight="661" class="origin\_image zh-lightbox-thumb" width="1084" data-original="https://pic3.zhimg.com/v2-cc8a149fd049cfe864b094cdfb034992\_r.jpg"/>
  
   !\[]\(https://pic3.zhimg.com/80/v2-cc8a149fd049cfe864b094cdfb034992_720w.webp) 
  
  相信如果你认真看完了以上内容，能很好地明白红色的式子，当然这里我们假设了执行a动作时，必然会进入下一个状态s‘，即传输概率为1。
  
  **4.9 Solving the Bellman Optimality Equation求解贝尔曼最优方程**
  
  贝尔曼最优方程是非线性的，通常而言没有固定的解法，有很多著名的迭代解法：
  
  *   Value Iteration 价值迭代
  *   Policy Iteration 策略迭代
  *   Q-learning
  *   Sarsa
  
  这个可以大家之后去多了解了解。
  
  **5 Conclusion**
  
  最后做个总结的话，这个应该是我研究最久的模型了，里面可能有很不到位的讲解之处，请大家谅解。09:23